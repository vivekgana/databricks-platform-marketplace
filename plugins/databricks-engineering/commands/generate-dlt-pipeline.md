# Generate DLT Pipeline Command

## Description
Generate Delta Live Tables (DLT) pipelines from specifications with automatic quality expectations, streaming support, and medallion architecture. Creates production-ready, declarative data pipelines.

## Usage
```bash
/databricks-engineering:generate-dlt-pipeline [spec-file] [--mode mode] [--enable-cdc]
```

## Parameters

- `spec-file` (required): Path to pipeline specification (YAML/JSON)
- `--mode` (optional): Pipeline mode - "batch", "streaming", "hybrid" (default: "batch")
- `--enable-cdc` (optional): Enable change data capture (default: false)
- `--enable-scd` (optional): Enable slowly changing dimensions (default: false)
- `--quality-level` (optional): "basic", "standard", "strict" (default: "standard")
- `--auto-optimize` (optional): Enable auto-optimization (default: true)

## Examples

### Example 1: Generate batch DLT pipeline
```bash
/databricks-engineering:generate-dlt-pipeline ./specs/customer-pipeline.yaml
```

### Example 2: Streaming pipeline with CDC
```bash
/databricks-engineering:generate-dlt-pipeline ./specs/events-pipeline.yaml \
  --mode streaming \
  --enable-cdc
```

### Example 3: Hybrid mode with SCD Type 2
```bash
/databricks-engineering:generate-dlt-pipeline ./specs/dimensions-pipeline.yaml \
  --mode hybrid \
  --enable-scd \
  --quality-level strict
```

## Pipeline Specification

### Example spec-file.yaml
```yaml
pipeline:
  name: customer_360_dlt
  target: customer_360
  storage: /mnt/dlt/customer_360
  configuration:
    spark.databricks.delta.preview.enabled: "true"
    pipelines.trigger.interval: "1 hour"

layers:
  bronze:
    - name: raw_customers
      source:
        type: cloudFiles
        format: json
        path: /mnt/raw/customers
        schema_location: /mnt/schemas/customers
      expectations:
        - name: valid_customer_id
          constraint: "customer_id IS NOT NULL"
          action: drop

  silver:
    - name: cleaned_customers
      source:
        table: LIVE.raw_customers
      transformations:
        - type: deduplicate
          key_columns: [customer_id]
        - type: standardize
          columns:
            email: lower(trim(email))
            phone: regexp_replace(phone, '[^0-9]', '')
        - type: enrich
          lookups:
            - table: ref_countries
              join_key: country_code
              select_columns: [country_name, region]
      expectations:
        - name: valid_email
          constraint: "email RLIKE '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$'"
          action: track
        - name: positive_age
          constraint: "age > 0 AND age < 150"
          action: drop

  gold:
    - name: customer_metrics
      source:
        table: LIVE.cleaned_customers
      transformations:
        - type: aggregate
          group_by: [customer_id]
          metrics:
            total_orders: "COUNT(order_id)"
            total_spent: "SUM(amount)"
            avg_order_value: "AVG(amount)"
            last_order_date: "MAX(order_date)"
      expectations:
        - name: valid_metrics
          constraint: "total_orders >= 0 AND total_spent >= 0"
          action: fail
```

## Generated DLT Pipeline

### Complete Pipeline Python File
```python
# pipelines/customer_360_dlt.py
"""
Delta Live Tables Pipeline: customer_360_dlt
Generated by databricks-engineering plugin
"""

import dlt
from pyspark.sql import functions as F
from pyspark.sql import Window


# ================================================================================
# BRONZE LAYER - Raw Ingestion with Basic Quality
# ================================================================================

@dlt.table(
    name="raw_customers",
    comment="Raw customer data from source systems",
    table_properties={
        "quality": "bronze",
        "pipelines.autoOptimize.zOrderCols": "customer_id",
        "delta.enableChangeDataFeed": "true"
    },
    temporary=False
)
@dlt.expect("valid_customer_id", "customer_id IS NOT NULL")
def raw_customers():
    """Ingest raw customer data using Auto Loader"""
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", "/mnt/schemas/customers")
        .option("cloudFiles.inferColumnTypes", "true")
        .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
        .load("/mnt/raw/customers")
        .withColumn("_ingestion_timestamp", F.current_timestamp())
        .withColumn("_source_file", F.input_file_name())
    )


# ================================================================================
# SILVER LAYER - Cleaned and Enriched Data
# ================================================================================

@dlt.table(
    name="cleaned_customers",
    comment="Cleaned and enriched customer data",
    table_properties={
        "quality": "silver",
        "pipelines.autoOptimize.zOrderCols": "customer_id,country_code"
    }
)
# Critical expectations - fail pipeline if violated
@dlt.expect_or_fail("unique_customer_id", "customer_id IS NOT NULL")

# High severity - drop invalid records
@dlt.expect_or_drop("valid_email_format", "email RLIKE '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}$'")
@dlt.expect_or_drop("valid_age_range", "age > 0 AND age < 150")
@dlt.expect_or_drop("valid_country", "country_code IS NOT NULL")

# Medium severity - track violations
@dlt.expect("reasonable_age", "age BETWEEN 18 AND 100")
@dlt.expect("valid_phone", "phone RLIKE '^[0-9]{10,15}$'")
def cleaned_customers():
    """Clean, deduplicate, and enrich customer data"""

    # Read from bronze
    bronze_df = dlt.read_stream("raw_customers")

    # Deduplicate
    window_spec = Window.partitionBy("customer_id").orderBy(F.col("_ingestion_timestamp").desc())
    deduped_df = (
        bronze_df
        .withColumn("_row_number", F.row_number().over(window_spec))
        .filter(F.col("_row_number") == 1)
        .drop("_row_number")
    )

    # Standardize data
    standardized_df = (
        deduped_df
        .withColumn("email", F.lower(F.trim(F.col("email"))))
        .withColumn("phone", F.regexp_replace(F.col("phone"), "[^0-9]", ""))
        .withColumn("first_name", F.initcap(F.trim(F.col("first_name"))))
        .withColumn("last_name", F.initcap(F.trim(F.col("last_name"))))
    )

    # Enrich with reference data
    ref_countries = dlt.read("ref_countries")
    enriched_df = (
        standardized_df
        .join(ref_countries, "country_code", "left")
        .select(
            standardized_df["*"],
            ref_countries["country_name"],
            ref_countries["region"]
        )
    )

    # Add processing metadata
    final_df = (
        enriched_df
        .withColumn("_processing_timestamp", F.current_timestamp())
        .withColumn("_data_version", F.lit("1.0"))
    )

    return final_df


# SCD Type 2 implementation
@dlt.table(
    name="customers_scd",
    comment="Customer dimension with SCD Type 2",
    table_properties={
        "quality": "silver",
        "delta.enableChangeDataFeed": "true"
    }
)
def customers_scd():
    """Implement Slowly Changing Dimension Type 2"""
    return (
        dlt.read_stream("cleaned_customers")
        .withColumn("effective_date", F.current_date())
        .withColumn("end_date", F.lit(None).cast("date"))
        .withColumn("is_current", F.lit(True))
    )


# ================================================================================
# GOLD LAYER - Business Aggregations and Metrics
# ================================================================================

@dlt.table(
    name="customer_metrics",
    comment="Aggregated customer metrics and KPIs",
    table_properties={
        "quality": "gold",
        "pipelines.autoOptimize.zOrderCols": "customer_id"
    }
)
@dlt.expect("valid_metrics", "total_orders >= 0 AND total_spent >= 0")
@dlt.expect("positive_ltv", "customer_lifetime_value > 0")
def customer_metrics():
    """Calculate customer-level metrics"""

    customers = dlt.read("cleaned_customers")
    orders = dlt.read("cleaned_orders")

    metrics_df = (
        customers
        .join(orders, "customer_id", "left")
        .groupBy(
            customers["customer_id"],
            customers["email"],
            customers["country_code"],
            customers["region"]
        )
        .agg(
            F.count("order_id").alias("total_orders"),
            F.sum("amount").alias("total_spent"),
            F.avg("amount").alias("avg_order_value"),
            F.max("order_date").alias("last_order_date"),
            F.min("order_date").alias("first_order_date"),
            F.countDistinct(F.col("order_date").cast("date")).alias("active_days")
        )
        .withColumn(
            "customer_lifetime_value",
            F.col("total_spent") + (F.col("avg_order_value") * 5)  # Simple LTV calculation
        )
        .withColumn(
            "customer_segment",
            F.when(F.col("total_spent") > 10000, "VIP")
            .when(F.col("total_spent") > 1000, "Gold")
            .when(F.col("total_spent") > 100, "Silver")
            .otherwise("Bronze")
        )
        .withColumn("calculation_date", F.current_date())
    )

    return metrics_df


@dlt.table(
    name="daily_customer_summary",
    comment="Daily rollup of customer activity"
)
def daily_customer_summary():
    """Daily customer activity summary"""
    return (
        dlt.read("customer_metrics")
        .groupBy("calculation_date", "region", "customer_segment")
        .agg(
            F.count("customer_id").alias("customer_count"),
            F.sum("total_orders").alias("total_orders"),
            F.sum("total_spent").alias("total_revenue"),
            F.avg("customer_lifetime_value").alias("avg_ltv")
        )
    )


# ================================================================================
# DATA QUALITY MONITORING
# ================================================================================

@dlt.table(
    name="data_quality_metrics",
    comment="Data quality metrics tracking"
)
def data_quality_metrics():
    """Track data quality metrics over time"""

    bronze_quality = dlt.read("raw_customers")
    silver_quality = dlt.read("cleaned_customers")

    return (
        spark.sql("""
            SELECT
                current_timestamp() as check_timestamp,
                'customer_360_dlt' as pipeline_name,
                'silver' as layer,
                COUNT(*) as total_records,
                SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_email_count,
                SUM(CASE WHEN phone IS NULL THEN 1 ELSE 0 END) as null_phone_count,
                SUM(CASE WHEN age < 18 OR age > 100 THEN 1 ELSE 0 END) as invalid_age_count
            FROM LIVE.cleaned_customers
        """)
    )


# ================================================================================
# REFERENCE DATA
# ================================================================================

@dlt.table(
    name="ref_countries",
    comment="Country reference data"
)
def ref_countries():
    """Load country reference data"""
    return (
        spark.read
        .format("delta")
        .load("/mnt/reference/countries")
    )
```

### Pipeline Configuration JSON
```json
{
  "id": "customer_360_dlt",
  "name": "Customer 360 DLT Pipeline",
  "storage": "/mnt/dlt/customer_360",
  "target": "customer_360",
  "continuous": false,
  "development": false,
  "photon": true,
  "libraries": [
    {
      "notebook": {
        "path": "/Workspace/pipelines/customer_360_dlt"
      }
    }
  ],
  "clusters": [
    {
      "label": "default",
      "autoscale": {
        "min_workers": 2,
        "max_workers": 8,
        "mode": "ENHANCED"
      },
      "custom_tags": {
        "Pipeline": "customer_360_dlt",
        "Environment": "production"
      }
    }
  ],
  "configuration": {
    "spark.databricks.delta.preview.enabled": "true",
    "pipelines.trigger.interval": "1 hour"
  },
  "email_notifications": {
    "alerts": ["data-engineering@company.com"],
    "on_failure": ["data-engineering@company.com", "oncall@company.com"]
  }
}
```

## Best Practices

1. **Layered Architecture**: Implement bronze/silver/gold pattern
2. **Quality Expectations**: Use appropriate action levels (track/drop/fail)
3. **Streaming**: Use streaming for real-time requirements
4. **SCD**: Implement Type 2 for slowly changing dimensions
5. **Monitoring**: Track quality metrics over time
6. **Testing**: Test with sample data before production
7. **Documentation**: Document expectations and transformations
8. **Optimization**: Enable auto-optimize and Z-ordering

## Troubleshooting

**Issue**: Pipeline fails with expectation violations
**Solution**: Review expectation constraints, adjust thresholds, use expect_or_drop instead of expect_or_fail

**Issue**: Slow performance with streaming
**Solution**: Adjust trigger interval, optimize cluster sizing, enable Photon

**Issue**: Schema evolution errors
**Solution**: Enable schema evolution in Auto Loader, use schema hints, test schema changes in dev

## Related Commands

- `/databricks-engineering:work-pipeline` - Traditional pipeline implementation
- `/databricks-engineering:test-data-quality` - Quality testing
- `/databricks-engineering:deploy-workflow` - Deploy DLT pipeline

---

**Last Updated**: 2024-12-31
**Version**: 1.0.0
**Category**: Pipeline Generation
**Prepared by**: Data Platform Team
